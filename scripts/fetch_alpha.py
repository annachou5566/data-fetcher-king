import json
import os
import time
import urllib.parse
from datetime import datetime, timedelta
from dotenv import load_dotenv
import requestsÂ 
import cloudscraper
import boto3Â 
from botocore.config import Config

# --- 1. Cáº¤U HÃŒNH ---
load_dotenv()

# Cáº¥u hÃ¬nh R2 / S3
R2_ACCESS_KEY_ID = os.getenv("R2_ACCESS_KEY_ID")
R2_SECRET_ACCESS_KEY = os.getenv("R2_SECRET_ACCESS_KEY")
R2_ENDPOINT_URL = os.getenv("R2_ENDPOINT_URL")
R2_BUCKET_NAME = os.getenv("R2_BUCKET_NAME")

PROXY_WORKER_URL = os.getenv("PROXY_WORKER_URL")
API_AGG_TICKER = os.getenv("BINANCE_INTERNAL_AGG_API")
API_AGG_KLINES = os.getenv("BINANCE_INTERNAL_KLINES_API")
API_PUBLIC_SPOT = "https://api.binance.com/api/v3/exchangeInfo"

ACTIVE_SPOT_SYMBOLS = set()
OLD_DATA_MAP = {}

# --- KHá»I Táº O Káº¾T Ná»I R2 (OBJECT STORAGE) ---
def get_r2_client():
Â  Â  if not R2_ACCESS_KEY_ID or not R2_SECRET_ACCESS_KEY:
Â  Â  Â  Â  print("âš ï¸ Thiáº¿u R2 Credentials! Kiá»ƒm tra GitHub Secrets.")
Â  Â  Â  Â  return None
Â  Â  return boto3.client(
Â  Â  Â  Â  's3',
Â  Â  Â  Â  endpoint_url=R2_ENDPOINT_URL,
Â  Â  Â  Â  aws_access_key_id=R2_ACCESS_KEY_ID,
Â  Â  Â  Â  aws_secret_access_key=R2_SECRET_ACCESS_KEY,
Â  Â  Â  Â  config=Config(signature_version='s3v4')
Â  Â  )

# --- KHá»I Táº O SESSION REQUESTS ---
session = cloudscraper.create_scraper(
Â  Â  browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)
session.headers.update({
Â  Â  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
Â  Â  "Referer": "https://www.binance.com/en/alpha",
Â  Â  "Origin": "https://www.binance.com",
Â  Â  "Accept": "application/json"
})

# --- MAPPING LÃ€M Rá»I Dá»® LIá»†U (Äáº¦Y Äá»¦ 100%) ---
KEY_MAP = {
Â  Â  "id": "i", "symbol": "s", "name": "n", "icon": "ic",
Â  Â  "chain": "cn", "chain_icon": "ci", "contract": "ct",
Â  Â  "status": "st", "price": "p", "change_24h": "c",
Â  Â  "market_cap": "mc", "liquidity": "l", "volume": "v",
Â  Â  "holders": "h",
Â  Â  "rolling_24h": "r24", "daily_total": "dt",
Â  Â  "daily_limit": "dl", "daily_onchain": "do",
Â  Â  "chart": "ch", "listing_time": "lt", "tx_count": "tx",
Â  Â  "offline": "off", "listingCex": "cex",
Â  Â  "onlineTge": "tge",
Â  Â  "onlineAirdrop": "air",
Â  Â  # [Má»šI] ThÃªm Mul Point
Â  Â  "mul_point": "mp"
}

def minify_token_data(token):
Â  Â  minified = {}
Â  Â  # 1. CÃ¡c trÆ°á»ng cÆ¡ báº£n
Â  Â  minified[KEY_MAP["id"]] = token.get("id")
Â  Â  minified[KEY_MAP["symbol"]] = token.get("symbol")
Â  Â  minified[KEY_MAP["name"]] = token.get("name")
Â  Â  minified[KEY_MAP["icon"]] = token.get("icon")
Â  Â Â 
Â  Â  # 2. CÃ¡c trÆ°á»ng Chain (Máº¡ng lÆ°á»›i) - ÄÃƒ Bá»” SUNG Äáº¦Y Äá»¦
Â  Â  minified[KEY_MAP["chain"]] = token.get("chain")Â  Â  Â  Â  Â  Â # TÃªn máº¡ng
Â  Â  minified[KEY_MAP["chain_icon"]] = token.get("chain_icon") # Logo máº¡ng (CÃ¡i báº¡n Ä‘ang tÃ¬m)
Â  Â  minified[KEY_MAP["contract"]] = token.get("contract")

Â  Â  # 3. Tráº¡ng thÃ¡i & GiÃ¡
Â  Â  minified[KEY_MAP["status"]] = token.get("status")
Â  Â  minified[KEY_MAP["price"]] = token.get("price")
Â  Â  minified[KEY_MAP["change_24h"]] = token.get("change_24h")
Â  Â  minified[KEY_MAP["mul_point"]] = token.get("mul_point")Â  Â # [Má»šI] Äiá»ƒm nhÃ¢n

Â  Â  # 4. Sá»‘ liá»‡u tÃ i chÃ­nh (Ã‰p kiá»ƒu int cho gá»n náº¿u sá»‘ lá»›n)
Â  Â  minified[KEY_MAP["market_cap"]] = int(token.get("market_cap", 0))
Â  Â  minified[KEY_MAP["holders"]] = int(token.get("holders", 0))
Â  Â  minified[KEY_MAP["liquidity"]] = int(token.get("liquidity", 0))
Â  Â  minified[KEY_MAP["tx_count"]] = int(token.get("tx_count", 0))
Â  Â Â 
Â  Â  # 5. ThÃ´ng tin Listing / Offline
Â  Â  minified[KEY_MAP["listing_time"]] = token.get("listing_time")
Â  Â  minified[KEY_MAP["offline"]] = 1 if token.get("offline") else 0
Â  Â  minified[KEY_MAP["listingCex"]] = 1 if token.get("listingCex") else 0
Â  Â  minified[KEY_MAP["onlineTge"]] = 1 if token.get("onlineTge") else 0
Â  Â  minified[KEY_MAP["onlineAirdrop"]] = 1 if token.get("onlineAirdrop") else 0

Â  Â  # 6. Volume (Giá»¯ nguyÃªn cáº¥u trÃºc object con)
Â  Â  vol = token.get("volume", {})
Â  Â  minified[KEY_MAP["volume"]] = {
Â  Â  Â  Â  KEY_MAP["rolling_24h"]: int(vol.get("rolling_24h", 0)),
Â  Â  Â  Â  KEY_MAP["daily_total"]: int(vol.get("daily_total", 0)),
Â  Â  Â  Â  KEY_MAP["daily_limit"]: int(vol.get("daily_limit", 0)),
Â  Â  Â  Â  KEY_MAP["daily_onchain"]: int(vol.get("daily_onchain", 0))
Â  Â  }
Â  Â Â 
Â  Â  # 7. Biá»ƒu Ä‘á»“
Â  Â  minified[KEY_MAP["chart"]] = token.get("chart", [])
Â  Â Â 
Â  Â  return minified

# --- HÃ€M Gá»ŒI API ---
def fetch_smart(target_url, retries=3):
Â  Â  is_render = "onrender.com" in (PROXY_WORKER_URL or "")
Â  Â  if not target_url or "None" in target_url: return None

Â  Â  for i in range(retries):
Â  Â  Â  Â  if PROXY_WORKER_URL:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  encoded_target = urllib.parse.quote(target_url, safe='')
Â  Â  Â  Â  Â  Â  Â  Â  proxy_final_url = f"{PROXY_WORKER_URL}?url={encoded_target}"
Â  Â  Â  Â  Â  Â  Â  Â  current_timeout = 60 if (is_render and i == 0) else 30
Â  Â  Â  Â  Â  Â  Â  Â  res = session.get(proxy_final_url, timeout=current_timeout)
Â  Â  Â  Â  Â  Â  Â  Â  if res.status_code == 200:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = res.json()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if isinstance(data, dict):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "symbols" in data: return dataÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if data.get("code") == "000000": return data
Â  Â  Â  Â  Â  Â  Â  Â  elif res.status_code == 502: time.sleep(3)
Â  Â  Â  Â  Â  Â  except: pass
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  res = session.get(target_url, timeout=15)
Â  Â  Â  Â  Â  Â  if res.status_code == 200:
Â  Â  Â  Â  Â  Â  Â  Â  data = res.json()
Â  Â  Â  Â  Â  Â  Â  Â  if "symbols" in data: return data
Â  Â  Â  Â  Â  Â  Â  Â  if data.get("code") == "000000": return data
Â  Â  Â  Â  except: pass
Â  Â  Â  Â  time.sleep(1)
Â  Â  return None

def safe_float(v):
Â  Â  try: return float(v) if v else 0.0
Â  Â  except: return 0.0

# --- Táº¢I DATA CÅ¨ Tá»ª R2 (THAY VÃŒ LOAD LOCAL) ---
def load_old_data_from_r2(r2_client):
Â  Â  if not r2_client: return {}
Â  Â  try:
Â  Â  Â  Â  # Táº£i file market-data.json tá»« R2 vá» bá»™ nhá»›
Â  Â  Â  Â  obj = r2_client.get_object(Bucket=R2_BUCKET_NAME, Key='market-data.json')
Â  Â  Â  Â  data = json.loads(obj['Body'].read().decode('utf-8'))
Â  Â  Â  Â Â 
Â  Â  Â  Â  # VÃ¬ dá»¯ liá»‡u cÅ© trÃªn R2 Ä‘Ã£ bá»‹ Minify (lÃ m rá»‘i), ta cáº§n map ngÆ°á»£c láº¡i ID
Â  Â  Â  Â  # Ä‘á»ƒ code logic hiá»ƒu Ä‘Æ°á»£c. (Tuy nhiÃªn, logic check limit chá»§ yáº¿u cáº§n ID,
Â  Â  Â  Â  # náº¿u minify ID váº«n giá»¯ nguyÃªn thÃ¬ OK).
Â  Â  Â  Â  # á» Ä‘Ã¢y Ä‘Æ¡n giáº£n hÃ³a: Náº¿u Ä‘Ã£ minify thÃ¬ ID lÃ  key "i"
Â  Â  Â  Â Â 
Â  Â  Â  Â  tokens = data.get('data', [])
Â  Â  Â  Â  mapped_data = {}
Â  Â  Â  Â  for t in tokens:
Â  Â  Â  Â  Â  Â  # Map key 'i' (minified) hoáº·c 'id' (legacy)
Â  Â  Â  Â  Â  Â  tid = t.get('i') or t.get('id')
Â  Â  Â  Â  Â  Â  if tid: mapped_data[tid] = t
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  return mapped_data
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"âš ï¸ KhÃ´ng táº£i Ä‘Æ°á»£c cache tá»« R2 (Láº§n Ä‘áº§u cháº¡y?): {e}")
Â  Â  Â  Â  return {}

def get_active_spot_symbols():
Â  Â  try:
Â  Â  Â  Â  print("â³ Check Spot Market...", end=" ", flush=True)
Â  Â  Â  Â  data = fetch_smart(API_PUBLIC_SPOT)
Â  Â  Â  Â  if data and "symbols" in data:
Â  Â  Â  Â  Â  Â  res = {s["baseAsset"] for s in data["symbols"] if s["status"] == "TRADING"}
Â  Â  Â  Â  Â  Â  print(f"OK ({len(res)})")
Â  Â  Â  Â  Â  Â  return res
Â  Â  except: pass
Â  Â  return set()

def fetch_details_optimized(chain_id, contract_addr):
Â  Â  if not API_AGG_KLINES: return 0, 0, 0, []
Â  Â  no_lower_chains = ["CT_501", "CT_784"]
Â  Â  clean_addr = str(contract_addr)
Â  Â  if chain_id not in no_lower_chains: clean_addr = clean_addr.lower()
Â  Â Â 
Â  Â  base_url = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=30&tokenAddress={clean_addr}"
Â  Â  d_total, d_limit = 0.0, 0.0
Â  Â  chart_data = []

Â  Â  try:
Â  Â  Â  Â  res_limit = fetch_smart(f"{base_url}&dataType=limit")
Â  Â  Â  Â  if res_limit and res_limit.get("data") and res_limit["data"].get("klineInfos"):
Â  Â  Â  Â  Â  Â  k_infos = res_limit["data"]["klineInfos"]
Â  Â  Â  Â  Â  Â  if k_infos: d_limit = safe_float(k_infos[-1][5])
Â  Â  except: pass

Â  Â  try:
Â  Â  Â  Â  res_agg = fetch_smart(f"{base_url}&dataType=aggregate")
Â  Â  Â  Â  if res_agg and res_agg.get("data") and res_agg["data"].get("klineInfos"):
Â  Â  Â  Â  Â  Â  k_infos = res_agg["data"]["klineInfos"]
Â  Â  Â  Â  Â  Â  if k_infos:
Â  Â  Â  Â  Â  Â  Â  Â  d_total = safe_float(k_infos[-1][5])
Â  Â  Â  Â  Â  Â  Â  Â  chart_data = [{"p": safe_float(k[4]), "v": safe_float(k[5])} for k in k_infos]
Â  Â  except: pass

Â  Â  d_market = d_total - d_limit
Â  Â  if d_market < 0: d_market = 0Â 
Â  Â  return d_total, d_limit, d_market, chart_data

def process_single_token(item):
Â  Â  aid = item.get("alphaId")
Â  Â  if not aid: return None

Â  Â  vol_rolling = safe_float(item.get("volume24h"))
Â  Â  symbol = item.get("symbol")
Â  Â  contract = item.get("contractAddress")
Â  Â  chain_id = item.get("chainId")
Â  Â  is_offline = item.get("offline", False)
Â  Â  is_listing_cex = item.get("listingCex", False)
Â  Â Â 
Â  Â  status = "ALPHA"
Â  Â  need_limit_check = FalseÂ 
Â  Â  if is_offline:
Â  Â  Â  Â  if is_listing_cex or symbol in ACTIVE_SPOT_SYMBOLS: status = "SPOT"
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  status = "PRE_DELISTED"
Â  Â  Â  Â  Â  Â  need_limit_check = True

Â  Â  # --- [Má»šI] LOGIC CACHE: CHáº¶N TOKEN ÄÃƒ CHáº¾T Tá»ª Láº¦N TRÆ¯á»šC ---
Â  Â  # Má»¥c Ä‘Ã­ch: Náº¿u lá»‹ch sá»­ (OLD_DATA_MAP) ghi nháº­n lÃ  DELISTED thÃ¬ bá» qua luÃ´n.
Â  Â  # LÆ°u Ã½: OLD_DATA_MAP dÃ¹ng key Ä‘Ã£ minify (vÃ­ dá»¥: KEY_MAP["status"] = "st")
Â  Â  if OLD_DATA_MAP and aid in OLD_DATA_MAP:
Â  Â  Â  Â  old_item = OLD_DATA_MAP[aid]
Â  Â  Â  Â  # Kiá»ƒm tra tráº¡ng thÃ¡i cÅ©
Â  Â  Â  Â  if old_item.get(KEY_MAP["status"]) == "DELISTED":
Â  Â  Â  Â  Â  Â  status = "DELISTED"
Â  Â  Â  Â  Â  Â  need_limit_check = FalseÂ  # Táº¯t cá» check limit Ä‘á»ƒ khÃ´ng chui vÃ o should_fetch
Â  Â  # -----------------------------------------------------------

Â  Â  should_fetch = False
Â  Â  if vol_rolling > 0 and (status == "ALPHA" or status == "PRE_DELISTED"):
Â  Â  Â  Â  should_fetch = True
Â  Â Â 
Â  Â  daily_total, daily_limit, daily_onchain = 0.0, 0.0, 0.0
Â  Â  chart_data = []
Â  Â Â 
Â  Â  # Logic Cache: Cáº§n xá»­ lÃ½ khÃ©o hÆ¡n vÃ¬ key cache Ä‘Ã£ bá»‹ minify
Â  Â  # NhÆ°ng Ä‘á»ƒ an toÃ n cho phiÃªn báº£n nÃ y, ta táº¡m Æ°u tiÃªn fetch má»›i.
Â  Â Â 
Â  Â  if should_fetch:
Â  Â  Â  Â  print(f"ğŸ“¡ {symbol}...", end=" ", flush=True)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  d_t, d_l, d_m, chart = fetch_details_optimized(chain_id, contract)
Â  Â  Â  Â  Â  Â  daily_total, daily_limit, daily_onchain = d_t, d_l, d_m
Â  Â  Â  Â  Â  Â  chart_data = chart
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if need_limit_check:
Â  Â  Â  Â  Â  Â  Â  Â  if daily_limit > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status = "ALPHA"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("âœ… ALIVE")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status = "DELISTED"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print("âŒ DEAD")
Â  Â  Â  Â  Â  Â  else: print("OK")
Â  Â  Â  Â  Â  Â  if daily_total <= 0: daily_total = vol_rolling
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Err: {e}")
Â  Â  Â  Â  Â  Â  daily_total = vol_rolling
Â  Â  Â  Â  Â  Â  if need_limit_check: status = "DELISTED"
Â  Â  else:
Â  Â  Â  Â  daily_total = vol_rolling
Â  Â  Â  Â  if status == "PRE_DELISTED": status = "DELISTED"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # [Má»šI] TÃ¡i sá»­ dá»¥ng Chart cÅ© náº¿u cÃ³ (Ä‘á»ƒ khÃ´ng bá»‹ máº¥t biá»ƒu Ä‘á»“ khi skip fetch)
Â  Â  Â  Â  if status == "DELISTED" and OLD_DATA_MAP and aid in OLD_DATA_MAP:
Â  Â  Â  Â  Â  Â  old_item = OLD_DATA_MAP[aid]
Â  Â  Â  Â  Â  Â  if old_item.get(KEY_MAP["chart"]):
Â  Â  Â  Â  Â  Â  Â  Â  chart_data = old_item.get(KEY_MAP["chart"])

Â  Â  return {
Â  Â  Â  Â  "id": aid, "symbol": symbol, "name": item.get("name"),
Â  Â  Â  Â  "icon": item.get("iconUrl"), "chain": item.get("chainName", ""),
Â  Â  Â  Â  "chain_icon": item.get("chainIconUrl"), "contract": contract,
Â  Â  Â  Â  "offline": is_offline, "listingCex": is_listing_cex, "status": status,
Â  Â  Â  Â  "onlineTge": item.get("onlineTge", False),
Â  Â  Â  Â  "onlineAirdrop": item.get("onlineAirdrop", False),
Â  Â  Â  Â  "mul_point": safe_float(item.get("mulPoint")),
Â  Â  Â  Â  "listing_time": item.get("listingTime", 0),
Â  Â  Â  Â  "tx_count": safe_float(item.get("count24h")),
Â  Â  Â  Â  "price": safe_float(item.get("price")),
Â  Â  Â  Â  "change_24h": safe_float(item.get("percentChange24h")),
Â  Â  Â  Â  "liquidity": safe_float(item.get("liquidity")),
Â  Â  Â  Â  "market_cap": safe_float(item.get("marketCap")),
Â  Â  Â  Â  "holders": safe_float(item.get("holders")),
Â  Â  Â  Â  "volume": {
Â  Â  Â  Â  Â  Â  "rolling_24h": vol_rolling, "daily_total": daily_total,
Â  Â  Â  Â  Â  Â  "daily_limit": daily_limit, "daily_onchain": daily_onchain
Â  Â  Â  Â  },
Â  Â  Â  Â  "chart": chart_data
Â  Â  }

# --- [Má»šI] THUáº¬T TOÃN TÃNH CÃI ÄUÃ”I 1440 PHÃšT ---
def build_suffix_sum(klines, yesterday_str):
Â  Â  arr = [0.0] * 1440
Â  Â  if not klines: return arr
Â  Â  minute_map = [0.0] * 1440
Â  Â Â 
Â  Â  for k in klines:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  candle_ts = int(k[0])
Â  Â  Â  Â  Â  Â  dt = datetime.utcfromtimestamp(candle_ts / 1000.0)
Â  Â  Â  Â  Â  Â  if dt.strftime('%Y-%m-%d') == yesterday_str:
Â  Â  Â  Â  Â  Â  Â  Â  start_min = dt.hour * 60 + dt.minute
Â  Â  Â  Â  Â  Â  Â  Â  vol_per_min = float(k[5] or 0) / 5.0
Â  Â  Â  Â  Â  Â  Â  Â  for i in range(5):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if start_min + i < 1440:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  minute_map[start_min + i] += vol_per_min
Â  Â  Â  Â  except: pass
Â  Â  Â  Â Â 
Â  Â  running_sum = 0.0
Â  Â  for i in range(1439, -1, -1):
Â  Â  Â  Â  running_sum += minute_map[i]
Â  Â  Â  Â  arr[i] = round(running_sum, 2) # LÃ m trÃ²n 2 sá»‘ tháº­p phÃ¢n Ä‘á»ƒ siÃªu nÃ©n JSON
Â  Â  return arr

def generate_and_upload_tails(r2_client, raw_tokens):
Â  Â  print("\nğŸ¦Š Báº¯t Ä‘áº§u quÃ©t CÃ¡i ÄuÃ´i 5m cho toÃ n thá»‹ trÆ°á»ng...")
Â  Â  yesterday_str = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
Â  Â Â 
Â  Â  tails_total, tails_limit = {}, {}
Â  Â  valid_tokens = [t for t in raw_tokens if safe_float(t.get("volume24h")) > 0]
Â  Â Â 
Â  Â  for idx, t in enumerate(valid_tokens):
Â  Â  Â  Â  aid = t.get("alphaId")
Â  Â  Â  Â  chain_id = t.get("chainId")
Â  Â  Â  Â  contract = t.get("contractAddress")
Â  Â  Â  Â  if not aid or not contract: continue
Â  Â  Â  Â Â 
Â  Â  Â  Â  if idx % 50 == 0: print(f"Â  Â Äang xá»­ lÃ½ {idx}/{len(valid_tokens)} token...")
Â  Â  Â  Â Â 
Â  Â  Â  Â  clean_addr = str(contract)
Â  Â  Â  Â  if chain_id not in ["CT_501", "CT_784"]: clean_addr = clean_addr.lower()
Â  Â  Â  Â Â 
Â  Â  Â  Â  base_url = f"{API_AGG_KLINES}?chainId={chain_id}&interval=5m&limit=1000&tokenAddress={clean_addr}"
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  res_tot = fetch_smart(f"{base_url}&dataType=aggregate", retries=1)
Â  Â  Â  Â  Â  Â  if res_tot and "data" in res_tot and "klineInfos" in res_tot["data"]:
Â  Â  Â  Â  Â  Â  Â  Â  tails_total[aid] = build_suffix_sum(res_tot["data"]["klineInfos"], yesterday_str)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  res_lim = fetch_smart(f"{base_url}&dataType=limit", retries=1)
Â  Â  Â  Â  Â  Â  if res_lim and "data" in res_lim and "klineInfos" in res_lim["data"]:
Â  Â  Â  Â  Â  Â  Â  Â  tails_limit[aid] = build_suffix_sum(res_lim["data"]["klineInfos"], yesterday_str)
Â  Â  Â  Â  except: pass
Â  Â  Â  Â  time.sleep(0.1) # LÃ¡ch Ban IP mÆ°á»£t mÃ 
Â  Â  Â  Â Â 
Â  Â  print("â˜ï¸ Äang Upload Tails lÃªn R2...")
Â  Â  json_str = json.dumps({"total": tails_total, "limit": tails_limit}, separators=(',', ':'))
Â  Â  try:
Â  Â  Â  Â  r2_client.put_object(Bucket=R2_BUCKET_NAME, Key='tails_cache.json', Body=json_str.encode('utf-8'), ContentType='application/json')
Â  Â  Â  Â  print("âœ… ÄÃ£ lÆ°u tails_cache.json thÃ nh cÃ´ng!")
Â  Â  except Exception as e: print(f"âŒ Upload Tails Failed: {e}")


def fetch_data():
Â  Â  global ACTIVE_SPOT_SYMBOLS, OLD_DATA_MAP
Â  Â  start = time.time()
Â  Â Â 
Â  Â  r2 = get_r2_client()
Â  Â  if not r2: return

Â  Â  OLD_DATA_MAP = load_old_data_from_r2(r2)
Â  Â  ACTIVE_SPOT_SYMBOLS = get_active_spot_symbols()
Â  Â Â 
Â  Â  print("â³ List...", end=" ", flush=True)
Â  Â  try: raw_res = fetch_smart(API_AGG_TICKER)
Â  Â  except: return
Â  Â  if not raw_res: return
Â  Â Â 
Â  Â  raw_data = raw_res.get("data", [])
Â  Â  print(f"Done ({len(raw_data)})")

Â  Â  target_tokens = raw_data
Â  Â  target_tokens.sort(key=lambda x: safe_float(x.get("volume24h")), reverse=True)
Â  Â Â 
Â  Â  results = []
Â  Â  print(f"ğŸš€ Processing {len(target_tokens)} Tokens (R2 Storage Mode)...")
Â  Â Â 
Â  Â  for t in target_tokens:
Â  Â  Â  Â  r = process_single_token(t)
Â  Â  Â  Â  if r: results.append(r)
        time.sleep(0.5)
Â  Â  results.sort(key=lambda x: x["volume"]["daily_total"], reverse=True)

Â  Â  # --- MINIFY DATA ---
Â  Â  print(f"ğŸ”’ Minifying...")
Â  Â  minified_results = [minify_token_data(t) for t in results]

Â  Â  final_output = {
Â  Â  Â  Â  "meta": {
Â  Â  Â  Â  Â  Â  "u": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
Â  Â  Â  Â  Â  Â  "t": len(minified_results),
Â  Â  Â  Â  Â  Â  "c": "WaveAlpha Data"
Â  Â  Â  Â  },
Â  Â  Â  Â  "data": minified_results
Â  Â  }
Â  Â Â 
Â  Â  json_str = json.dumps(final_output, ensure_ascii=False, separators=(',', ':'))

Â  Â  # --- UPLOAD TO CLOUDFLARE R2 ---
Â  Â  print("â˜ï¸ Uploading to Cloudflare R2...")
Â  Â  try:
Â  Â  Â  Â  # 1. Upload File Má»›i Nháº¥t
Â  Â  Â  Â  r2.put_object(
Â  Â  Â  Â  Â  Â  Bucket=R2_BUCKET_NAME,
Â  Â  Â  Â  Â  Â  Key='market-data.json',
Â  Â  Â  Â  Â  Â  Body=json_str.encode('utf-8'),
Â  Â  Â  Â  Â  Â  ContentType='application/json',
Â  Â  Â  Â  Â  Â  CacheControl='max-age=60' # Cache 1 phÃºt
Â  Â  Â  Â  )
Â  Â  Â  Â  print("âœ… Uploaded market-data.json")

Â  Â  Â  Â  # 2. Upload File Lá»‹ch Sá»­
Â  Â  Â  Â  today_str = datetime.now().strftime("%Y-%m-%d")
Â  Â  Â  Â  r2.put_object(
Â  Â  Â  Â  Â  Â  Bucket=R2_BUCKET_NAME,
Â  Â  Â  Â  Â  Â  Key=f'history/{today_str}.json',
Â  Â  Â  Â  Â  Â  Body=json_str.encode('utf-8'),
Â  Â  Â  Â  Â  Â  ContentType='application/json'
Â  Â  Â  Â  )
Â  Â  Â  Â  print(f"âœ… Uploaded history/{today_str}.json")

Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"âŒ R2 Upload Failed: {e}")
        
        generate_and_upload_tails(r2, target_tokens)

Â  Â  print(f"ğŸ DONE! Total: {time.time()-start:.1f}s")

if __name__ == "__main__":
Â  Â  fetch_data()
